{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random\nfrom pathlib import Path\nimport math","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-05-11T21:03:08.270245Z","iopub.execute_input":"2023-05-11T21:03:08.271150Z","iopub.status.idle":"2023-05-11T21:03:08.279291Z","shell.execute_reply.started":"2023-05-11T21:03:08.271088Z","shell.execute_reply":"2023-05-11T21:03:08.278205Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"myseed = 1091102  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:03:08.281526Z","iopub.execute_input":"2023-05-11T21:03:08.281884Z","iopub.status.idle":"2023-05-11T21:03:08.291764Z","shell.execute_reply.started":"2023-05-11T21:03:08.281850Z","shell.execute_reply":"2023-05-11T21:03:08.290834Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def padding(ball_pos , f_range , cond , front = True):\n    x = []\n    x = ball_pos[cond]\n\n    for _ in range(f_range - len(x)):\n\n        pad = pd.DataFrame([[0,0,0,0,0]],columns= ball_pos.columns)\n        if front == True:\n            ball_pos = pd.concat([pad , ball_pos])\n        else:\n            ball_pos = pd.concat([ball_pos , pad])\n\n    return ball_pos","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:03:08.293239Z","iopub.execute_input":"2023-05-11T21:03:08.293798Z","iopub.status.idle":"2023-05-11T21:03:08.301150Z","shell.execute_reply.started":"2023-05-11T21:03:08.293765Z","shell.execute_reply":"2023-05-11T21:03:08.300194Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class BadmintonDataset(Dataset):\n\n    def __init__(self,path,tfm,ball_path,files = None):\n        super(BadmintonDataset).__init__()\n        self.path = path\n        \n        if files != None:\n            self.files = files\n        else:\n            self.files = [] \n            for i in Path(path).glob('*'):\n                self.files.append(i)\n        self.ball_pos = pd.read_csv(ball_path)\n        self.transform = tfm\n        \n    \n    \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = str(self.files[idx])\n        im = Image.open(fname)\n        im = self.transform(im)\n        \n        f_range = 30\n        frame = int(fname.split(\"_\")[-2].strip('.'))\n        vid = int(fname.split('/')[-1].split(\"_\")[-3].strip('.'))\n        label = int(fname.split(\"_\")[-1].strip('.jpg'))\n        \n        if label == 1: \n            label = 0\n        elif label ==2:\n            label = 1\n        \n        cond = (self.ball_pos['VideoName'] == vid) & (abs(self.ball_pos['Frame'] - frame) <= f_range)  \n        ball_pos = self.ball_pos[cond]\n        \n        if len(ball_pos) == 0 or frame < 2:\n            ball_pos = pd.concat([pd.DataFrame([[0,frame,0,0,0]],columns= ball_pos.columns) , ball_pos])\n            \n        prev_cond = ((frame - ball_pos['Frame']) <= f_range) & ((frame - ball_pos['Frame']) > 0)\n        ball_pos = padding(ball_pos , f_range , prev_cond)\n        follow_cond = ((ball_pos['Frame'] - frame) <= f_range) & ((ball_pos['Frame'] - frame) > 0)\n        ball_pos = padding(ball_pos , f_range , follow_cond , front=False)\n            \n        ball_pos = torch.tensor(ball_pos[['Visibility','X','Y']].values , dtype = torch.float)\n        \n#         print(fname , len(ball_pos))\n\n        if torch.flatten(ball_pos).size(0) != 183 :\n#             print(img.shape ,ball.size(0) , label)\n            print(fname)\n                 \n        return im , torch.flatten(ball_pos) ,label\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:03:08.303788Z","iopub.execute_input":"2023-05-11T21:03:08.304269Z","iopub.status.idle":"2023-05-11T21:03:08.320906Z","shell.execute_reply.started":"2023-05-11T21:03:08.304169Z","shell.execute_reply":"2023-05-11T21:03:08.319797Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self , nb_classes):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn = models.efficientnet_v2_s(weights = models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        self.cnn_fc = nn.Linear(1000 , nb_classes)\n        \n        self.ball_prefc = nn.LazyLinear(100)\n        self.ball_attention = nn.TransformerEncoderLayer(100 , 1 , dim_feedforward = 128 , dropout=0.15 , batch_first = True)\n        self.ball_fc = nn.LazyLinear(nb_classes)\n        self.fin_fc = nn.LazyLinear(nb_classes)\n        \n    def forward(self, img , ball):\n        \n        imgout = self.cnn(img)\n        imgout = self.cnn_fc(imgout)\n        \n        ballout = self.ball_prefc(ball)\n        ballout = self.ball_attention(ballout)\n        ballout = self.ball_fc(ballout)\n        out = self.fin_fc(torch.cat((imgout,ballout) , dim = 1))\n        \n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:03:33.283410Z","iopub.execute_input":"2023-05-11T21:03:33.283760Z","iopub.status.idle":"2023-05-11T21:03:33.294674Z","shell.execute_reply.started":"2023-05-11T21:03:33.283730Z","shell.execute_reply":"2023-05-11T21:03:33.293758Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# image_size = (440,310)\nimage_size = (330,150)\ntest_tfm = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n])\n\ntrain_tfm = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.RandomAdjustSharpness(1.5, p=0.5),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.RandomAutocontrast(p=0.5),\n    transforms.ColorJitter(0.2,0.2,0.2,0.05),\n    transforms.ToTensor(),\n    transforms.RandomErasing(p=0.5,scale=(0.005,0.015),value=(1,1,1)),\n])","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:04:16.001786Z","iopub.execute_input":"2023-05-11T21:04:16.002792Z","iopub.status.idle":"2023-05-11T21:04:16.020502Z","shell.execute_reply.started":"2023-05-11T21:04:16.002741Z","shell.execute_reply":"2023-05-11T21:04:16.019183Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize a model, and put it on the device specified.\nnb_classes = 2\nmodel = Classifier(nb_classes).to(device)\nbatch_size = 32\nn_epochs = 20\npatience = 10\ncriterion = nn.CrossEntropyLoss(label_smoothing = 0.05)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\ntrain_valid_ratio = 0.9\n_exp_name = \"hit_model\"\n\n# read_model = \"/kaggle/input/models/sample_best.ckpt\"\n\n# if read_model != None:\n#     model.load_state_dict(torch.load(read_model))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:04:17.547569Z","iopub.execute_input":"2023-05-11T21:04:17.547921Z","iopub.status.idle":"2023-05-11T21:04:18.153775Z","shell.execute_reply.started":"2023-05-11T21:04:17.547891Z","shell.execute_reply":"2023-05-11T21:04:18.151824Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n","output_type":"stream"}]},{"cell_type":"code","source":"train_set = BadmintonDataset(\"/kaggle/input/badminton/dataset/dataset/train/images\" , tfm=train_tfm , ball_path=\"/kaggle/input/badminton/csv/all_ball_pos.csv\")\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = BadmintonDataset(\"/kaggle/input/badminton/dataset/dataset/valid/images\", tfm=test_tfm , ball_path=\"/kaggle/input/badminton/csv/all_ball_pos.csv\")\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\nprint(len(train_set))\nprint(len(valid_set))\n\nimg , ball , label = valid_set[0]\n\nprint(img.shape ,ball.shape, label)\nprint(model(img.unsqueeze(0).to(device) , ball.unsqueeze(0).to(device)))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:06:28.875637Z","iopub.execute_input":"2023-05-11T21:06:28.875998Z","iopub.status.idle":"2023-05-11T21:06:29.188137Z","shell.execute_reply.started":"2023-05-11T21:06:28.875970Z","shell.execute_reply":"2023-05-11T21:06:29.187192Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"14332\n2780\ntorch.Size([3, 330, 150]) torch.Size([183]) 0\ntensor([[ 0.0672, -0.4168]], device='cuda:0', grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0\ntrain_acc_record = []\ntrain_loss_record = []\nvalid_acc_record = []\nvalid_loss_record = []\n\nfor epoch in range(n_epochs):\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs , ball_pos , labels = batch\n        #imgs = imgs.half()\n        #print(imgs.shape,labels.shape)\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        logits = model(imgs.to(device) , ball_pos.to(device))\n        \n        _, preds = torch.max(logits, 1)\n        for t, p in zip(labels.view(-1), preds.view(-1)):\n            confusion_matrix[t.long(), p.long()] += 1\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = criterion(logits, labels.to(device))\n\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n        \n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n    \n    train_acc_record.append(train_acc.to('cpu'))\n    train_loss_record.append(train_loss)\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n    print(confusion_matrix)\n    print(\"All Class Acc\")\n    print(confusion_matrix.diag()/confusion_matrix.sum(1))\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, ball_pos, labels = batch\n        #imgs = imgs.half()\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(imgs.to(device) , ball_pos.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n        _, preds = torch.max(logits, 1)\n        for t, p in zip(labels.view(-1), preds.view(-1)):\n            confusion_matrix[t.long(), p.long()] += 1\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n        #break\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n    \n    valid_acc_record.append(valid_acc.to('cpu'))\n    valid_loss_record.append(valid_loss)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n    print(confusion_matrix)\n    print(\"All Class Acc\")\n    print(confusion_matrix.diag()/confusion_matrix.sum(1))\n\n    # update logs\n    if valid_acc > best_acc:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n    else:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # save models\n    if valid_acc > best_acc:\n        print(f\"Best model found at epoch {epoch}, saving model\")\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n        best_acc = valid_acc\n        stale = 0\n    else:\n        stale += 1\n        if stale > patience:\n            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n            break","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:04:29.968295Z","iopub.execute_input":"2023-05-11T21:04:29.968704Z","iopub.status.idle":"2023-05-11T21:05:38.379516Z","shell.execute_reply.started":"2023-05-11T21:04:29.968672Z","shell.execute_reply":"2023-05-11T21:05:38.376703Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/448 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"683818b5bb5a45f4a47879e5e9d1f800"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m train_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(nb_classes, nb_classes)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# A batch consists of image data and corresponding labels.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     imgs , ball_pos , labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#imgs = imgs.half()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#print(imgs.shape,labels.shape)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Forward the data. (Make sure data and model are on the same device.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mBadmintonDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles[idx])\n\u001b[1;32m     23\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(fname)\n\u001b[0;32m---> 24\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m f_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     27\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(fname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1287\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1287\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:976\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    974\u001b[0m     _log_api_usage_once(adjust_hue)\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:117\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    114\u001b[0m     np_h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(hue_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    115\u001b[0m h \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np_h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHSV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:1077\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     dither \u001b[38;5;241m=\u001b[39m Dither\u001b[38;5;241m.\u001b[39mFLOYDSTEINBERG\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1077\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot([*range(1,len(train_acc_record)+1)] , train_acc_record , label = \"training\")\nplt.plot([*range(1,len(train_acc_record)+1)] , valid_acc_record , label = \"validation\")\n\nplt.xticks(np.arange(0, n_epochs+1, 5))\nplt.legend(loc=\"upper left\")\n\nplt.savefig('acc.png')\nplt.show()\n\n\nplt.plot([*range(1,len(train_acc_record)+1)] , train_loss_record , label = \"training\")\nplt.plot([*range(1,len(train_acc_record)+1)] , valid_loss_record , label = \"valiidation\")\n\nplt.xticks(np.arange(0, n_epochs+1, 5))\nplt.legend(loc=\"upper left\")\n\nplt.savefig('loss.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T21:03:08.485192Z","iopub.status.idle":"2023-05-11T21:03:08.486242Z","shell.execute_reply.started":"2023-05-11T21:03:08.485911Z","shell.execute_reply":"2023-05-11T21:03:08.485936Z"},"trusted":true},"execution_count":null,"outputs":[]}]}