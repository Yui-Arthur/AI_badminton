{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random\nfrom pathlib import Path","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-05-10T17:01:40.106337Z","iopub.execute_input":"2023-05-10T17:01:40.107227Z","iopub.status.idle":"2023-05-10T17:01:43.175407Z","shell.execute_reply.started":"2023-05-10T17:01:40.107187Z","shell.execute_reply":"2023-05-10T17:01:43.172887Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"myseed = 1091102  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T17:01:47.954665Z","iopub.execute_input":"2023-05-10T17:01:47.955069Z","iopub.status.idle":"2023-05-10T17:01:47.963728Z","shell.execute_reply.started":"2023-05-10T17:01:47.955032Z","shell.execute_reply":"2023-05-10T17:01:47.962349Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class BadmintonDataset(Dataset):\n\n    def __init__(self,path,tfm,files = None):\n        super(BadmintonDataset).__init__()\n        self.path = path\n        \n        if files != None:\n            self.files = files\n        else:\n            self.files = [] \n            for i in Path(path).glob('*'):\n                self.files.append(i)\n            \n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = str(self.files[idx])\n        im = Image.open(fname)\n        im = self.transform(im)\n        \n        label = int(fname.split(\"_\")[-1].strip('.jpg'))\n            \n        return im,label","metadata":{"execution":{"iopub.status.busy":"2023-05-10T17:01:49.400036Z","iopub.execute_input":"2023-05-10T17:01:49.400526Z","iopub.status.idle":"2023-05-10T17:01:49.410953Z","shell.execute_reply.started":"2023-05-10T17:01:49.400487Z","shell.execute_reply":"2023-05-10T17:01:49.409647Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn = models.efficientnet_v2_s(weights = models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n        self.fc = nn.Linear(1000 , 3)\n        \n    def forward(self, x):\n        out = self.cnn(x)\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-05-10T17:01:52.170544Z","iopub.execute_input":"2023-05-10T17:01:52.171247Z","iopub.status.idle":"2023-05-10T17:01:52.178418Z","shell.execute_reply.started":"2023-05-10T17:01:52.171206Z","shell.execute_reply":"2023-05-10T17:01:52.177215Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# image_size = (440,310)\nimage_size = (330,150)\ntest_tfm = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n])\n\ntrain_tfm = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.RandomAdjustSharpness(1.5, p=0.5),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.RandomAutocontrast(p=0.5),\n    transforms.ColorJitter(0.2,0.2,0.2,0.05),\n    transforms.ToTensor(),\n    transforms.RandomErasing(p=0.5,scale=(0.005,0.015),value=(1,1,1)),\n])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T17:01:54.406772Z","iopub.execute_input":"2023-05-10T17:01:54.407826Z","iopub.status.idle":"2023-05-10T17:01:54.416663Z","shell.execute_reply.started":"2023-05-10T17:01:54.407774Z","shell.execute_reply":"2023-05-10T17:01:54.415181Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize a model, and put it on the device specified.\nmodel = Classifier().to(device)\nbatch_size = 32\nn_epochs = 20\npatience = 10\ncriterion = nn.CrossEntropyLoss(label_smoothing = 0.05)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\ntrain_valid_ratio = 0.9\n_exp_name = \"hit_model\"\nnb_classes = 3\n# read_model = \"/kaggle/input/models/sample_best.ckpt\"\n\n# if read_model != None:\n#     model.load_state_dict(torch.load(read_model))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T17:01:56.822816Z","iopub.execute_input":"2023-05-10T17:01:56.823573Z","iopub.status.idle":"2023-05-10T17:02:02.594642Z","shell.execute_reply.started":"2023-05-10T17:01:56.823532Z","shell.execute_reply":"2023-05-10T17:02:02.593501Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/82.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282d1842c7ae4ecfae1718b29c399344"}},"metadata":{}}]},{"cell_type":"code","source":"train_set = BadmintonDataset(\"/kaggle/input/badminton/dataset/dataset/train/images\", tfm=train_tfm )\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = BadmintonDataset(\"/kaggle/input/badminton/dataset/dataset/valid/images\", tfm=test_tfm )\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\nprint(len(train_set))\nprint(len(valid_set))\n\n_ , label = valid_set[0]\nprint(_.shape , label)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T17:02:13.406780Z","iopub.execute_input":"2023-05-10T17:02:13.407202Z","iopub.status.idle":"2023-05-10T17:02:13.629666Z","shell.execute_reply.started":"2023-05-10T17:02:13.407161Z","shell.execute_reply":"2023-05-10T17:02:13.628520Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"14332\n2780\ntorch.Size([3, 330, 150]) 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0\ntrain_acc_record = []\ntrain_loss_record = []\nvalid_acc_record = []\nvalid_loss_record = []\n\nfor epoch in range(n_epochs):\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n        #print(imgs.shape,labels.shape)\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        logits = model(imgs.to(device))\n        \n        _, preds = torch.max(logits, 1)\n        for t, p in zip(labels.view(-1), preds.view(-1)):\n            confusion_matrix[t.long(), p.long()] += 1\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = criterion(logits, labels.to(device))\n\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n        \n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n    \n    train_acc_record.append(train_acc.to('cpu'))\n    train_loss_record.append(train_loss)\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n    print(confusion_matrix)\n    print(\"All Class Acc\")\n    print(confusion_matrix.diag()/confusion_matrix.sum(1))\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(imgs.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n        _, preds = torch.max(logits, 1)\n        for t, p in zip(labels.view(-1), preds.view(-1)):\n            confusion_matrix[t.long(), p.long()] += 1\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n        #break\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n    \n    valid_acc_record.append(valid_acc.to('cpu'))\n    valid_loss_record.append(valid_loss)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n    print(confusion_matrix)\n    print(\"All Class Acc\")\n    print(confusion_matrix.diag()/confusion_matrix.sum(1))\n\n    # update logs\n    if valid_acc > best_acc:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n    else:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # save models\n    if valid_acc > best_acc:\n        print(f\"Best model found at epoch {epoch}, saving model\")\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n        best_acc = valid_acc\n        stale = 0\n    else:\n        stale += 1\n        if stale > patience:\n            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n            break","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:21:09.044826Z","iopub.execute_input":"2023-05-04T07:21:09.045677Z","iopub.status.idle":"2023-05-04T07:30:43.260320Z","shell.execute_reply.started":"2023-05-04T07:21:09.045637Z","shell.execute_reply":"2023-05-04T07:30:43.258590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot([*range(1,len(train_acc_record)+1)] , train_acc_record , label = \"training\")\nplt.plot([*range(1,len(train_acc_record)+1)] , valid_acc_record , label = \"validation\")\n\nplt.xticks(np.arange(0, n_epochs+1, 5))\nplt.legend(loc=\"upper left\")\n\nplt.savefig('acc.png')\nplt.show()\n\n\nplt.plot([*range(1,len(train_acc_record)+1)] , train_loss_record , label = \"training\")\nplt.plot([*range(1,len(train_acc_record)+1)] , valid_loss_record , label = \"valiidation\")\n\nplt.xticks(np.arange(0, n_epochs+1, 5))\nplt.legend(loc=\"upper left\")\n\nplt.savefig('loss.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T07:30:43.261455Z","iopub.status.idle":"2023-05-04T07:30:43.262415Z","shell.execute_reply.started":"2023-05-04T07:30:43.262134Z","shell.execute_reply":"2023-05-04T07:30:43.262160Z"},"trusted":true},"execution_count":null,"outputs":[]}]}